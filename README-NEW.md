# cross modal retrieval based on vision mamba.

This is a collection of resources related to cross modal retrieval&mamba.

![PRs Welcome](https://img.shields.io/badge/PRs-Welcome-green) [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

# Contents

- [baselines](#baselines)
- [数据集](#数据集)
- [Paperlist](#Papers)
  - [1 Survey](#Surveys)
  - [2 Cross modal retrieval](#retrieval)
     - [2.1 2021](#2021)
     - [2.2 2022](#2022)
     - [2.3 2023](#2023)
     - [2.4 2024](#2024)
- [模型解构](#model)
- [Codes](#Codes)
  
<a name="baselines" />

# baselines
|title|publication&date|summary|cost|recommendation|
|---|---|---|---|---|
||||||

<a name="数据集" />

#数据集

![数据集](https://cdn.nlark.com/yuque/0/2023/png/25419362/1687616552109-46cc65ca-d84b-4d70-b6c5-5c07acac4f00.png#averageHue=%23f7f5f4&clientId=uce2552bf-d2ca-4&from=paste&height=438&id=u20e72a73&originHeight=547&originWidth=1317&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=56362&status=done&style=none&taskId=uf65a463e-5eb9-462e-b39c-494757e84e5&title=&width=1053.6)

<a name="Papers" />

# Paperlist

<a name="surveys" />

## 1 Surveys

1. **Image-text Retrieval: A Survey on Recent Research and Development.** *Min Cao, Shiping Li, Juntao Li, Liqiang Nie, Min Zhang.* 2022 [survey] (https://arxiv.org/abs/2203.14713)
2. **Cross-Modal Retrieval: A Systematic Review of Methods and Future Directions.** *Fengling Li, Lei Zhu, Tianshi Wang, Jingjing Li, Zheng Zhang, Heng Tao Shen* 2023 [survey] (https://arxiv.org/abs/2308.14263)
3. **The State of the Art for Cross-Modal Retrieval: A Survey.** *Kun Zhou; Fadratul Hafinaz Hassan; Gan Keng Hoon* 2023 [survey]  (https://ieeexplore.ieee.org/abstract/document/10336787)
4. **Multimodal Learning with Transformers: A Survey.** *Peng Xu; Xiatian Zhu; David A.* 2023 [survey] (https://ieeexplore.ieee.org/document/10123038)
5. 

<a name="retrieval" />

## 2 retrieval

<a name="2021" />

### 2.1 2021

1. 

<a name="2022" />

### 2.2 2022

1. 


<a name="2023" />

### 2.3 2023

1. 

<a name="2024" />

### 2.4 2024

1. **Augment the Pairs: Semantics-Preserving Image-Caption Pair Augmentation for Grounding-Based Vision and Language Models.** *Jingru Yi, Burak Uzkent, Oana Ignat, Zili Li, Amanmeet Garg, Xiang Yu, Linda Liu* 2024 [paper] (https://arxiv.org/abs/2311.02536)
2. **SeTformer is What You Need for Vision and Language.** *Pourya Shamsolmoali, Masoumeh Zareapoor, Eric Granger, Michael Felsberg* 2024 [paper] [vision and language transformer] (https://arxiv.org/abs/2401.03540)


<a name="model" />

# 模型解构

## 主流多模态检索技术路线
![ca1d9625fb9da876a5353df9ef186cb](https://github.com/lpf992/vision-mamba/assets/151422800/82136247-9e56-4218-aaa4-d978a3356a84)
- 单流
- 双流*

<a name="Codes" />

# Codes







    
