# cross modal retrieval based on vision mamba.

This is a collection of resources related to cross modal retrieval&mamba.

![PRs Welcome](https://img.shields.io/badge/PRs-Welcome-green) [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

# Contents

- [baselines](#baselines)
- [Paperlist](#Papers)
  - [1 Survey](#Surveys)
  - [2 Cross modal retrieval](#retrieval)
     - [2.1 2021](#2021)
     - [2.2 2022](#2022)
     - [2.3 2023](#2023)
     - [2.4 2024](#2024)
- [Codes](#Codes)
  
<a name="baselines" />

# baselines
|title|publication&date|summary|cost|recommendation|
|---|---|---|---|---|
||||||



<a name="Papers" />

# Paperlist

<a name="surveys" />

## 1 Surveys

1. **Image-text Retrieval: A Survey on Recent Research and Development.** *Min Cao, Shiping Li, Juntao Li, Liqiang Nie, Min Zhang.* 2022 [survey] (https://arxiv.org/abs/2203.14713)
2. **Cross-Modal Retrieval: A Systematic Review of Methods and Future Directions.** *Fengling Li, Lei Zhu, Tianshi Wang, Jingjing Li, Zheng Zhang, Heng Tao Shen* 2023 [survey] (https://arxiv.org/abs/2308.14263)
3. **The State of the Art for Cross-Modal Retrieval: A Survey.** *Kun Zhou; Fadratul Hafinaz Hassan; Gan Keng Hoon* 2023 [survey]  (https://ieeexplore.ieee.org/abstract/document/10336787)
4. **Multimodal Learning with Transformers: A Survey.** *Peng Xu; Xiatian Zhu; David A.* 2023 [survey] (https://ieeexplore.ieee.org/document/10123038)
5. 

<a name="retrieval" />

## 2 retrieval

<a name="2021" />

### 2.1 2021

1. **GFlowNets for AI-driven scientific discovery.** *Jain, M., Deleu, T., Hartford, J., Liu, C. H., Hernandez-Garcia, A., & Bengio, Y.* 2023 [paper] [done] (https://pubs.rsc.org/en/content/articlehtml/2023/dd/d3dd00002h)
2. **Reconstructing porous media using generative flow networks.** *Guan, K. M., Anderson, T. I., Creux, P., & Kovscek, A. R.* 2021 [paper] (https://www.sciencedirect.com/science/article/pii/S0098300421001965)
3. 

<a name="2022" />

### 2.2 2022

1. **Generative flow networks for discrete probabilistic modeling.** *Zhang, D., Malkin, N., Liu, Z., Volokhova, A., Courville, A., & Bengio, Y.* 2022 [paper] [done] (https://proceedings.mlr.press/v162/zhang22v.html)
2. 


<a name="2023" />

### 2.3 2023

1. **Batchgfn: Generative flow networks for batch active learning.** *Malik, S. A., Lahlou, S., Jesson, A., Jain, M., Malkin, N., Deleu, T., ... & Gal, Y.* 2023 [paper] (https://arxiv.org/abs/2306.15058)
2. 

<a name="2024" />

### 2.4 2024

1. **Augment the Pairs: Semantics-Preserving Image-Caption Pair Augmentation for Grounding-Based Vision and Language Models.** *Jingru Yi, Burak Uzkent, Oana Ignat, Zili Li, Amanmeet Garg, Xiang Yu, Linda Liu* 2024 [paper] (https://arxiv.org/abs/2311.02536)
2. **SeTformer is What You Need for Vision and Language.** *Pourya Shamsolmoali, Masoumeh Zareapoor, Eric Granger, Michael Felsberg* 2024 [paper] (https://arxiv.org/abs/2401.03540)

<a name="Codes" />

# Codes







    
